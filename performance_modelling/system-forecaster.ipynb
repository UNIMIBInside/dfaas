{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to fill NaN values within the dataframe X\n",
    "def fill_NaN(X):\n",
    "  for col in X:\n",
    "    if(col.startswith('success_rate_')):\n",
    "      X.loc[:, col] = X.loc[:, col].fillna(1)\n",
    "    else:\n",
    "      X.loc[:, col] = X.loc[:, col].fillna(0)\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reweight of dataframe\n",
    "def resample_dataset(X, y):\n",
    "  X_resampled, y_resampled = resample(X, y, replace=True, random_state=42)\n",
    "  return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to split the dataset into training and test set\n",
    "def split_dataset(X, y):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "  print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(X_train, X_test, y_train, y_test):\n",
    "  X_train_nn = X_train.astype(np.float32)\n",
    "  X_test_nn = X_test.astype(np.float32)\n",
    "  y_train_nn = y_train.astype(np.float32)\n",
    "  y_test_nn = y_test.astype(np.float32)\n",
    "  \n",
    "  return X_train_nn, X_test_nn, y_train_nn, y_test_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to calculate the weighted mean squared error\n",
    "def wmse_score(y_true, y_pred):\n",
    "  # Calculates the weight of classes for the first target  \n",
    "  median_cpu = y_true['cpu_usage_node'].median()\n",
    "  w_majority_cpu = y_true[y_true['cpu_usage_node'] <= median_cpu].shape[0] / y_true.shape[0]\n",
    "  w_minority_cpu = y_true[y_true['cpu_usage_node'] > median_cpu].shape[0] / y_true.shape[0]\n",
    "\n",
    "  # Calculate the weight of classes for the second target\n",
    "  median_ram = y_true['ram_usage_node'].median()\n",
    "  w_majority_ram = y_true[y_true['ram_usage_node'] <= median_ram].shape[0] / y_true.shape[0]\n",
    "  w_minority_ram = y_true[y_true['ram_usage_node'] > median_ram].shape[0] / y_true.shape[0]\n",
    "\n",
    "  # Calculates the MSE for both targets\n",
    "  mse_cpu = mean_squared_error(y_true['cpu_usage_node'], y_pred['cpu_usage_node'])\n",
    "  mse_ram = mean_squared_error(y_true['ram_usage_node'], y_pred['ram_usage_node'])\n",
    "\n",
    "  # Calculates WMSE as a weighted average of the MSEs for the two targets\n",
    "  wmse = (w_majority_cpu * mse_cpu * y_true.shape[0] / (w_majority_cpu * y_true[y_true['cpu_usage_node'] <= median_cpu].shape[0] + w_minority_cpu * y_true[y_true['cpu_usage_node'] > median_cpu].shape[0]) +\n",
    "          w_majority_ram * mse_ram * y_true.shape[0] / (w_majority_ram * y_true[y_true['ram_usage_node'] <= median_ram].shape[0] + w_minority_ram * y_true[y_true['ram_usage_node'] > median_ram].shape[0])) / 2\n",
    "\n",
    "  return wmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to calculate metrics based on the task\n",
    "def metrics(task_type, y_test, y_pred):\n",
    "  if(task_type == 'regression'):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"mse:\", mse)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(\"R-squared score:\", r2)\n",
    "    return mse, r2\n",
    "    \n",
    "  elif(task_type == 'mo-classification'):\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: {}'.format(accuracy))\n",
    "    \n",
    "  elif(task_type == 'classification'):\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to plot the regression lines for the 2 targets\n",
    "def plot_regression(y_test, y_pred):\n",
    "  # Extract the values of cpu_usage_node and ram_usage_node from the dataFrame y_test\n",
    "  y1_test = y_test['cpu_usage_node']\n",
    "  y2_test = y_test['ram_usage_node']\n",
    "\n",
    "  # Extract the cpu_usage_node and ram_usage_node values from the dataFrame y_pred\n",
    "  y1_pred = y_pred['cpu_usage_node']\n",
    "  y2_pred = y_pred['ram_usage_node']\n",
    "\n",
    "  # Calculate the regression lines\n",
    "  m1, q1 = np.polyfit(y1_test, y1_pred, 1)\n",
    "  m2, q2 = np.polyfit(y2_test, y2_pred, 1)\n",
    "\n",
    "  # Plot the regression lines\n",
    "  plt.plot(y1_test, y1_pred, 'o', color='red', fillstyle='none', label='Utilizzo CPU')\n",
    "  plt.plot(y1_test, m1*y1_test + q1, linestyle='--',  label='Regressione uso CPU')\n",
    "  plt.plot(y2_test, y2_pred, '+', label='Utilizzo RAM')\n",
    "  plt.plot(y2_test, m2*y2_test + q2, color= 'black', linestyle='-',  label='Regressione uso RAM')\n",
    "  plt.xlabel('Valori osservati')\n",
    "  plt.ylabel('Valori predetti')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_test, y_pred, target):\n",
    "  # Calculate the confusion matrix\n",
    "  cm = confusion_matrix(y_test[target], y_pred[target])\n",
    "\n",
    "  # Plot the confusion matrix as heatmap\n",
    "  sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "  plt.xlabel('Valori osservati')\n",
    "  plt.ylabel('Valori predetti')\n",
    "  plt.title('Confusion matrix')\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all files in the output folder\n",
    "file_csv = [file for file in os.listdir('output') if file.endswith('.csv')]\n",
    "\n",
    "# Create the dataframe by concatenating all read files\n",
    "dataframes = []\n",
    "for file in file_csv:\n",
    "    file_path = os.path.join('output', file)\n",
    "    dataframes.append(pd.read_csv(file_path))\n",
    "df = pd.concat(dataframes)\n",
    "\n",
    "# Remove the columns in the dataframe that begin with \"function_\"\n",
    "to_drop = [df.drop(col, axis=1, inplace=True) for col in df if col.startswith('function_')]\n",
    "df = fill_NaN(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU and RAM as targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe division by features and output\n",
    "targets = [col for col in df if col.endswith('_usage_node')]\n",
    "params = [col for col in df if not col.endswith('_usage_node')]\n",
    "X = df[params]\n",
    "y = df[targets]\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X = fill_NaN(X)\n",
    "X_scaled = MinMaxScaler().fit_transform(X, y)\n",
    "y_scaled = MinMaxScaler().fit_transform([[val1, val2] for val1, val2 in zip(y['cpu_usage_node'], y['ram_usage_node'])])\n",
    "X_train, X_test, y_train, y_test = split_dataset(X_scaled, y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Gradient Boosting model\n",
    "gb = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, random_state=42))\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Performance evaluation of the model on the test set\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "y_pred_gb = pd.DataFrame(y_pred_gb, columns=['cpu_usage_node', 'ram_usage_node'])\n",
    "y_test = pd.DataFrame(y_test, columns=['cpu_usage_node', 'ram_usage_node'])\n",
    "\n",
    "metrics('regression', y_test, y_pred_gb)\n",
    "\n",
    "plot_regression(y_test, y_pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Random Forest model\n",
    "rf = MultiOutputRegressor(RandomForestRegressor(n_estimators=10, random_state=42))\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Performance evaluation of the model on the test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_rf = pd.DataFrame(y_pred_rf, columns=['cpu_usage_node', 'ram_usage_node'])\n",
    "\n",
    "metrics('regression', y_test, y_pred_rf)\n",
    "\n",
    "plot_regression(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Neural Network model\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = convert_dataset(X_train, X_test, y_train, y_test)\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Dense(64, input_dim=X_train_nn.shape[1], activation='relu'))\n",
    "nn.add(Dense(32, activation='relu'))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(16, activation='relu'))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(2, activation='linear'))\n",
    "\n",
    "nn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "nn.fit(X_train_nn, y_train_nn, batch_size=32,epochs=100, validation_data=(X_test_nn, y_test_nn), callbacks=[early_stop])\n",
    "\n",
    "# Performance evaluation of the model on the test set\n",
    "y_pred_nn = nn.predict(X_test_nn)\n",
    "y_pred_nn = pd.DataFrame(y_pred_nn, columns=['cpu_usage_node', 'ram_usage_node'])\n",
    "\n",
    "metrics('regression', y_test_nn, y_pred_nn)\n",
    "\n",
    "plot_regression(y_test_nn, y_pred_nn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-output Classification Task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU and RAM as targets discretized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe split by features and output\n",
    "targets = [col for col in df if col.endswith('_usage_node')]\n",
    "params = [col for col in df if not col.endswith('_usage_node')]\n",
    "X = df[params]\n",
    "y = df[targets]\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X = fill_NaN(X)\n",
    "X_scaled = MinMaxScaler().fit_transform(X, y)\n",
    "y_scaled = MinMaxScaler().fit_transform([[val1, val2] for val1, val2 in zip(y['cpu_usage_node'], y['ram_usage_node'])])\n",
    "y_scaled = pd.DataFrame(y_scaled, columns=['cpu_usage_node', 'ram_usage_node'])\n",
    "\n",
    "# Discretization of targets\n",
    "y_scaled.loc[:, ('cpu_usage_node')] = pd.cut(y_scaled.loc[:, ('cpu_usage_node')], 3).cat.codes\n",
    "y_scaled.loc[:, ('ram_usage_node')] = pd.cut(y_scaled.loc[:, ('ram_usage_node')], 6).cat.codes\n",
    "\n",
    "# Transformation of target classes into binary values\n",
    "y_scaled_cpu_dummies = pd.get_dummies(y_scaled['cpu_usage_node'], prefix='cpu_usage_node')\n",
    "y_scaled_ram_dummies = pd.get_dummies(y_scaled['ram_usage_node'], prefix='ram_usage_node')\n",
    "\n",
    "# Merge original dataframe with the one with binary columns\n",
    "y_scaled = pd.concat([y_scaled, y_scaled_cpu_dummies], axis=1)\n",
    "y_scaled = pd.concat([y_scaled, y_scaled_ram_dummies], axis=1)\n",
    "\n",
    "# Removing original columns\n",
    "y_scaled = y_scaled.drop('cpu_usage_node', axis=1)\n",
    "y_scaled = y_scaled.drop('ram_usage_node', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_dataset(X_scaled, y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Gradient Boosting model\n",
    "gb = MultiOutputClassifier(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, random_state=42))\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "metrics('mo-classification', y_test, y_pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Random Forest model\n",
    "rf = MultiOutputClassifier(RandomForestClassifier(n_estimators=10, random_state=42))\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "metrics('mo-classification', y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Neural Network model\n",
    "nn = Sequential()\n",
    "nn.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(32, activation='relu'))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(16, activation='relu'))\n",
    "nn.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "nn.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop])\n",
    "\n",
    "y_pred_nn = nn.predict(X_test)\n",
    "y_pred_nn = (y_pred_nn > 0.5).astype(int)\n",
    "\n",
    "metrics('mo-classification', y_test, y_pred_nn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overloaded Node as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe split by features and output\n",
    "targets = [col for col in df if 'overloaded_node' in col]\n",
    "params = [col for col in df if not 'overloaded_node' in col]\n",
    "\n",
    "X = df[params]\n",
    "y = df[targets]\n",
    "y = y['overloaded_node'].values.ravel()\n",
    "y = y.astype('int')\n",
    "\n",
    "# Oversampling\n",
    "sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)\n",
    "X, y = sm.fit_resample(X, y)\n",
    "Counter(y)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X = fill_NaN(X)\n",
    "X_scaled = MinMaxScaler().fit_transform(X, y)\n",
    "X_train, X_test, y_train, y_test = split_dataset(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "metrics('classification', y_test, y_pred_gb)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_gb)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Valori osservati')\n",
    "plt.ylabel('Valori predetti')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "metrics('classification', y_test, y_pred_rf)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Valori osservati')\n",
    "plt.ylabel('Valori predetti')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Neural Network model\n",
    "nn = Sequential()\n",
    "nn.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(32, activation='relu'))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(16, activation='relu'))\n",
    "nn.add(Dense(1, activation=\"sigmoid\"))\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "\n",
    "nn.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stop])\n",
    "nn.summary()\n",
    "\n",
    "y_pred_nn = nn.predict(X_test)\n",
    "y_pred_nn = (y_pred_nn > 0.5).astype(int)\n",
    "\n",
    "metrics('classification', y_test, y_pred_nn)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_nn)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Valori osservati')\n",
    "plt.ylabel('Valori predetti')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
